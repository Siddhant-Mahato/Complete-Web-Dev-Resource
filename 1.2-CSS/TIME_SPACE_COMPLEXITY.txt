Here's a summary of the time and space complexity of various data structures and sorting algorithms:

Data Structures:

Array:

Time:
Access: O(1)
Search: O(n)
Insertion: O(n) (at the end), O(n) (at the beginning or middle)
Deletion: O(n) (at the end), O(n) (at the beginning or middle)
Space: O(n)

Stack:

Time:
Push: O(1)
Pop: O(1)
Peek: O(1)
Space: O(n)

Queue:

Time:
Enqueue: O(1)
Dequeue: O(1)
Peek: O(1)
Space: O(n)


Linked List:

Time:
Access: O(n)
Search: O(n)
Insertion: O(1)
Deletion: O(1)
Space: O(n)


Hash Table:

Time:
Insertion: O(1) (average), O(n) (worst case)
Search: O(1) (average), O(n) (worst case)
Deletion: O(1) (average), O(n) (worst case)
Space: O(n)
Tree:

Time:
Search: O(log n) (balanced), O(n) (worst case)
Insertion: O(log n) (balanced), O(n) (worst case)
Deletion: O(log n) (balanced), O(n) (worst case)
Space: O(n)


Graph:

Time:
Search: O(V + E), where V is the number of vertices and E is the number of edges
Insertion: O(1)
Deletion: O(1)
Space: O(V + E)


Sorting Algorithms:

Bubble Sort:

Time: O(n^2)
Space: O(1)

Selection Sort:

Time: O(n^2)
Space: O(1)

Insertion Sort:

Time: O(n^2)
Space: O(1)


Merge Sort:

Time: O(n log n)
Space: O(n)


Quick Sort:

Time: O(n log n) (average), O(n^2) (worst case)
Space: O(log n) (average), O(n) (worst case)


Heap Sort:

Time: O(n log n)
Space: O(1)


Radix Sort:

Time: O(nk), where k is the number of digits in the largest number
Space: O(n + k)
Note: The time complexity of these algorithms depends on the specific implementation and can vary based on the input. The complexities listed here are in the best case